<!-- OPTIONAL: Project Logo
<p align="center">
  <img src="[OPT FILL: Path/link to logo image]" alt="Logo" style="width: 15%; display: block; margin: auto;">
</p>
-->

<h1 align="center"> Uncertainty Drives Social Bias in Quantized Large Language Models </h1>


<!-- OPTIONAL: Badges with Hyperlinks
<p align="center">
  <a href="[OPT FILL: Path/link to paper]"><img src="https://img.shields.io/badge/arXiv-2405.01535-b31b1b.svg" alt="arXiv"></a>
  <a href="[OPT FILL: Path/link to HuggingFace]"><img src="https://img.shields.io/badge/Hugging%20Face-Organization-ff9d00" alt="Hugging Face Organization"></a>
  <a href="[OPT FILL: Path/link to LICENSE]"><img src="https://img.shields.io/license-MIT-blue/license-MIT-blue.svg" alt="License"></a>
  <a href="[OPT FILL: Path/link to PyPI project]"><img src="https://img.shields.io/pypi/v/[OPT FILL: Name of PyPI package].svg" alt="PyPI version"></a>
</p>
-->

<p align="center">
  ‚ö° A repository for benchmarking social bias in LLMs post-quantization üöÄ ‚ö° <br>
</p>

---

## üå≤ About the Repo

<!-- OPTIONAL: Create Repository Structure Automatically
pip install rptree
rptree -d .
[OPT FILL: Copy structure to this README]
-->

```shell
./
‚îú‚îÄ‚îÄ data/                   # Data directory
‚îÇ   ‚îú‚îÄ‚îÄ ceb_datasets/            # CEB datasets
‚îÇ   ‚îú‚îÄ‚îÄ closed_datasets/         # Closed-ended datasets
‚îÇ   ‚îú‚îÄ‚îÄ open_datasets/           # Open-ended datasets
‚îÇ   ‚îî‚îÄ‚îÄ save_data/              # Saved artifacts from inference
‚îÇ       ‚îú‚îÄ‚îÄ llm_generations/        # Contains responses generated by each model
‚îÇ       ‚îú‚îÄ‚îÄ analysis/               # Contains analysis related data
‚îÇ       ‚îî‚îÄ‚îÄ models/                 # Contains local models
‚îú‚îÄ‚îÄ slurm/                  # Contains sbatch scripts for running on SLURM server
‚îÇ   ‚îî‚îÄ‚îÄ logs/                   # Stores logs from SLURM jobs
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ bin/                # Contains a script for renaming models in the command-line
‚îÇ   ‚îú‚îÄ‚îÄ utils/              # Contains important utility functions for running LLMs locally or through APIs
‚îÇ   ‚îî‚îÄ‚îÄ deprecated/         # Contains deprecated code
‚îú‚îÄ‚îÄ scripts/                # Contains scripts to run
‚îî‚îÄ‚îÄ config.py               # Contains global constants
```



## üí¥ About the Data

**Data Description**:

In this repository, we repackage the following datasets:

|  Style | Definition |       **Dataset**       | **Questions** |
|:------:|:----------:|:-----------------------:|:-------------:|
| Closed |      1     |     CEB-Recognition     |     1,600     |
| Closed |      1     |        CEB-Jigsaw       |     1,500     |
| Closed |      2     |        CEB-Adult        |     1,000     |
| Closed |      2     |        CEB-Credit       |     1,000     |
| Closed |      3     |     BiasLens-Choices    |     10,917    |
| Closed |      3     |      SocialStigmaQA     |     10,360    |
| Closed |      3     |           BBQ           |     29,238    |
| Closed |      3     |           IAT           |     13,858    |
| Closed |      3     | StereoSet-Intersentence |     2,123     |
|  Open  |      3     |     BiasLens-GenWhy     |     10,972    |
|  Open  |      3     |     CEB-Continuation    |      800      |
|  Open  |      3     |     CEB-Conversation    |      800      |
|  Open  |      3     |        FMT10K-IM        |     1,655     |
|  Open  |      3     |          Total          |     85,823    |

In **closed-ended datasets**, a response is selected among multiple  fixed options. We use geometric average tokene probability in each choice to select a response.

In **open-ended datasets**, a text response is generated with greedy decoding and evaluated later asynchronously using LLaMA Guard 8B. 


---

## üîß Installation

**(Manual) Installation:**
```shell
# Get repository
git clone https://github.com/[repository]
cd [repository]

# Create new conda environment
conda create --name fairbench python=3.10.15
conda activate fairbench

# Install dependencies
pip install -r envs/requirements.txt
```

## ‚öôÔ∏è Configuration

**(OpenAI) Registering your OpenAI key**
```shell
# Add to ~/.bashrc file (or to your .envrc)
echo 'export OPENAI_KEY="[ENTER HERE]"' >> ~/.bashrc

# Reload shell
source ~/.bashrc
```

**Adding a New Model**

| To add a new model, please update `MODEL_INFO` in `config.py`. Take, for example, "Meta-Llama-3.1-8B-Instruct-GPTQ-4bit".

| First, update `MODEL_INFO['model_group']` if it's a new model.
* Append `llama3.1-8b-instruct`.

| Next, in `MODEL_INFO['model_path_to_name']`, provide a mapping of the HuggingFace / local path to a model shorthand, following the standard: `[original_model]-[q_method]-[bit_configuration]`.
* Add "Meta-Llama-3.1-8B-Instruct-GPTQ-4bit" -> "llama3.1-8b-instruct-gptq-w4a16",



## üèÉ How to Run

**(Shell) Generate responses to all datasets for a single model**
```shell
# Option 1. In shell
# MODEL_NAME="meta-llama/Llama-3.1-8B-Instruct"
MODEL_NICKNAME="llama3.1-8b-instruct"    # shorthand defined in config.py / MODEL_INFO
python -m scripts.benchmark generate ${MODEL_NICKNAME};

# Option 2. In a SLURM batch job
# TODO: First, modify `slurm/generate.sh` to run the model specified
sbatch slurm/generate.sh
```


**(Shell) Classify safety of open-ended responses**
```shell
# Option 1. In shell
# MODEL_NAME="meta-llama/Llama-3.1-8B-Instruct"
MODEL_NICKNAME="llama3.1-8b-instruct"    # shorthand defined in config.py / MODEL_INFO
python -m scripts.benchmark bias_evaluate ${MODEL_NAME};

# Option 2. In a SLURM batch job
# TODO: First, modify `slurm/generate.sh` to run the model specified
sbatch slurm/eval_bias.sh
```


**(Shell) Perform analysis to reproduce paper results**
```shell
# Option 1. In shell
# MODEL_NAME="meta-llama/Llama-3.1-8B-Instruct"
MODEL_NICKNAME="llama3.1-8b-instruct"    # shorthand defined in config.py / MODEL_INFO
python -m scripts.benchmark bias_evaluate ${MODEL_NAME};

# Option 2. In a SLURM batch job
# TODO: First, modify `slurm/generate.sh` to run the model specified
sbatch slurm/eval_bias.sh
```


## üëè Acknowledgements

Special thanks to the authors of the [CEB Benchmark]((https://arxiv.org/pdf/2407.02408)), whose code base served
as the starting point for this repository.

**Collaborators**:
1. [Anonymous Snail](mailto:)


## Citation

If you find our work useful, please consider citing our paper!

```bibtex
@article{YourName,
  title={Your Title},
  author={Your team},
  journal={Location},
  year={Year}
}
```